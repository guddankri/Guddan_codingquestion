{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement the K-nearest neighbors (KNN) algorithm from scratch. Given a\n",
    " dataset and a query point, classify the query based on the majority label of its k-nearest\n",
    " neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for query point [5 5] is: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "def knn_classify(X_train, y_train, query_point, k=3):\n",
    "    distances = []\n",
    "    for i, data_point in enumerate(X_train):\n",
    "        dist = euclidean_distance(data_point, query_point)\n",
    "        distances.append((dist, y_train[i]))\n",
    "\n",
    "    distances = sorted(distances, key=lambda x: x[0])[:k]\n",
    "    k_nearest_labels = [label for _, label in distances]\n",
    "    majority_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "    \n",
    "    return majority_label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8], [8, 9]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "    query_point = np.array([5, 5])\n",
    "    k = 3\n",
    "    predicted_label = knn_classify(X_train, y_train, query_point, k)\n",
    "    \n",
    "    print(f\"Predicted label for query point {query_point} is: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement logistic regression in Python. Include training using gradient descent,\n",
    " and calculate class probabilities for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_predicted_proba = self.predict_proba(X)\n",
    "        return [1 if i >= threshold else 0 for i in y_predicted_proba]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import datasets\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    bc = datasets.load_breast_cancer()\n",
    "    X, y = bc.data, bc.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(learning_rate=0.01, n_iters=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to perform matrix multiplication without using external libraries\n",
    " like NumPy. Multiply two matrices representing weight matrices and input vectors in a neural\n",
    " network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 64]\n",
      "[139, 154]\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    # Get the dimensions of the matrices\n",
    "    rows_A = len(A)\n",
    "    cols_A = len(A[0])\n",
    "    rows_B = len(B)\n",
    "    cols_B = len(B[0])\n",
    "\n",
    "    # Check if multiplication is possible\n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Number of columns in A must be equal to number of rows in B.\")\n",
    "\n",
    "    # Initialize result matrix C with zeros\n",
    "    C = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            for k in range(cols_A):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "\n",
    "    return C\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example matrices (weights and input)\n",
    "    A = [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "    \n",
    "    B = [[7, 8],\n",
    "         [9, 10],\n",
    "         [11, 12]]\n",
    "\n",
    "    result = matrix_multiply(A, B)\n",
    "    for row in result:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a Python function that deep copies a neural network represented as a\n",
    " nested list of layers, where each layer contains weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original network:\n",
      "[[[0.2, 0.8], [0.5, 0.1]], [[0.6, 0.4, 0.9], [0.3, 0.7, 0.2]]]\n",
      "\n",
      "Copied network:\n",
      "[[[99, 0.8], [0.5, 0.1]], [[0.6, 0.4, 0.9], [0.3, 0.7, 0.2]]]\n"
     ]
    }
   ],
   "source": [
    "def deep_copy_network(network):\n",
    "    if isinstance(network, list):\n",
    "        return [deep_copy_network(layer) for layer in network]\n",
    "    else:\n",
    "        return network\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    original_network = [\n",
    "        [[0.2, 0.8], [0.5, 0.1]],  \n",
    "        [[0.6, 0.4, 0.9], [0.3, 0.7, 0.2]]  \n",
    "    ]\n",
    "    \n",
    "    copied_network = deep_copy_network(original_network)\n",
    "    copied_network[0][0][0] = 99\n",
    "\n",
    "    print(\"Original network:\")\n",
    "    print(original_network)\n",
    "    print(\"\\nCopied network:\")\n",
    "    print(copied_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a moving average filter for a time series. Given a series of numbers\n",
    " and a window size, return the moving average of the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving averages: [20.0, 30.0, 40.0, 50.0, 60.0]\n"
     ]
    }
   ],
   "source": [
    "def moving_average(series, window_size):\n",
    "    if window_size <= 0:\n",
    "        raise ValueError(\"Window size must be positive\")\n",
    "    \n",
    "    moving_averages = []\n",
    "    for i in range(len(series) - window_size + 1):\n",
    "        window = series[i:i + window_size]\n",
    "        window_avg = sum(window) / window_size\n",
    "        moving_averages.append(window_avg)\n",
    "    \n",
    "    return moving_averages\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    series = [10, 20, 30, 40, 50, 60, 70]\n",
    "    window_size = 3\n",
    "    result = moving_average(series, window_size)\n",
    "    print(\"Moving averages:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Write a Python function that applies the ReLU activation function to a list of\n",
    " inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU applied to inputs: [0, 0, 0, 2, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "def relu(inputs):\n",
    "    return [max(0, x) for x in inputs]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    inputs = [-3, -1, 0, 2, 5, -7]\n",
    "    result = relu(inputs)\n",
    "    print(\"ReLU applied to inputs:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement gradient descent to train a linear regression model. Minimize the\n",
    " mean squared error by adjusting the model weights iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 417.00933221477027\n",
      "Model weights: [46.73020522]\n",
      "Model bias: 0.18967596020864966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.datasets import make_regression\n",
    "    X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    regressor = LinearRegressionGD(learning_rate=0.01, n_iters=1000)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    mse = np.mean((y_test - y_pred) ** 2)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Model weights: {regressor.weights}\")\n",
    "    print(f\"Model bias: {regressor.bias}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to perform one-hot encoding of categorical labels for a\n",
    " machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded matrix:\n",
      "[0, 0, 1]\n",
      "[1, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 1]\n",
      "[1, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 1]\n",
      "\n",
      "Label mapping:\n",
      "{'dog': 0, 'bird': 1, 'cat': 2}\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels):\n",
    "    # Get the unique classes in the labels\n",
    "    unique_labels = list(set(labels))\n",
    "    \n",
    "    # Create a dictionary mapping each label to an index\n",
    "    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Create the one-hot encoded matrix\n",
    "    one_hot_encoded = []\n",
    "    for label in labels:\n",
    "        one_hot_vector = [0] * len(unique_labels)\n",
    "        one_hot_vector[label_to_index[label]] = 1\n",
    "        one_hot_encoded.append(one_hot_vector)\n",
    "    \n",
    "    return one_hot_encoded, label_to_index\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    labels = ['cat', 'dog', 'bird', 'cat', 'dog', 'bird', 'cat']\n",
    "    one_hot_matrix, label_mapping = one_hot_encode(labels)\n",
    "    \n",
    "    print(\"One-hot encoded matrix:\")\n",
    "    for row in one_hot_matrix:\n",
    "        print(row)\n",
    "    \n",
    "    print(\"\\nLabel mapping:\")\n",
    "    print(label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement a Python function to calculate the cosine similarity between two\n",
    " vectors, which is often used in text similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9746\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(\"Vectors must be of the same length\")\n",
    "    \n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    magnitude_vec1 = math.sqrt(sum(a**2 for a in vec1))\n",
    "    magnitude_vec2 = math.sqrt(sum(b**2 for b in vec2))\n",
    "    \n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        raise ValueError(\"One of the vectors has zero magnitude\")\n",
    "    \n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vec1 = [1, 2, 3]\n",
    "    vec2 = [4, 5, 6]\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    print(f\"Cosine similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a trained neural network in Python, write a function to prune the network\n",
    " by removing neurons that have small or zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned weights:\n",
      "[array([0.1, 0. , 0.3]), array([0.5, 0.7, 0. ])]\n",
      "[array([0.2, 0.1])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prune_neural_network(weights, threshold=1e-5):\n",
    "    pruned_weights = []\n",
    "    \n",
    "    for layer in weights:\n",
    "        pruned_layer = []\n",
    "        for neuron_weights in layer:\n",
    "            if np.linalg.norm(neuron_weights) > threshold:\n",
    "                pruned_layer.append(neuron_weights)\n",
    "        pruned_weights.append(pruned_layer)\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example weights for a simple neural network\n",
    "    weights = [\n",
    "        [np.array([0.1, 0.0, 0.3]), np.array([0.0, 0.0, 0.0]), np.array([0.5, 0.7, 0.0])],\n",
    "        [np.array([0.0, 0.0]), np.array([0.2, 0.1])]\n",
    "    ]\n",
    "    \n",
    "    pruned = prune_neural_network(weights, threshold=1e-5)\n",
    "    print(\"Pruned weights:\")\n",
    "    for layer in pruned:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Python function to compute the confusion matrix given true and\n",
    " predicted labels for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 1 0]\n",
      " [1 1 0]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    matrix = np.zeros((len(unique_labels), len(unique_labels)), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[true][pred] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    y_true = np.array([0, 1, 2, 2, 0, 1, 0])\n",
    "    y_pred = np.array([0, 0, 2, 2, 0, 1, 1])\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a Python function that performs mini-batch gradient descent for optimizing\n",
    " the weights of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights:\n",
      "[-0.02950649  0.45059196 -0.04269402 -0.02379824 -0.10494612  0.59043156\n",
      " -0.09994939  0.22188668 -0.05927275  0.06480126]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mini_batch_gradient_descent(X, y, weights, learning_rate=0.01, batch_size=32, epochs=100):\n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            predictions = np.dot(X_batch, weights)\n",
    "            errors = predictions - y_batch\n",
    "            gradient = np.dot(X_batch.T, errors) / X_batch.shape[0]\n",
    "            weights -= learning_rate * gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X = np.random.rand(100, 10)\n",
    "    y = np.random.rand(100)\n",
    "    initial_weights = np.random.rand(10)\n",
    "\n",
    "    optimized_weights = mini_batch_gradient_descent(X, y, initial_weights, learning_rate=0.01, batch_size=16, epochs=50)\n",
    "    print(\"Optimized weights:\")\n",
    "    print(optimized_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement k-means clustering from scratch. Given a dataset and the number of\n",
    " clusters \n",
    "k , return the cluster assignments for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments:\n",
      "[0. 0. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Randomly initialize the centroids\n",
    "    centroids = X[np.random.choice(num_samples, k, replace=False)]\n",
    "    cluster_assignments = np.zeros(num_samples)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Assign clusters\n",
    "        for i in range(num_samples):\n",
    "            distances = np.linalg.norm(X[i] - centroids, axis=1)\n",
    "            cluster_assignments[i] = np.argmin(distances)\n",
    "\n",
    "        # Update centroids\n",
    "        new_centroids = np.array([X[cluster_assignments == j].mean(axis=0) for j in range(k)])\n",
    "        \n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "    k = 2\n",
    "    \n",
    "    assignments = kmeans(X, k)\n",
    "    print(\"Cluster assignments:\")\n",
    "    print(assignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement a Python function to calculate the softmax of a list of numbers, which\n",
    " is used in multi-class classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax probabilities:\n",
      "[0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scores = [2.0, 1.0, 0.1]\n",
    "    probabilities = softmax(scores)\n",
    "    print(\"Softmax probabilities:\")\n",
    "    print(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to compute the TF-IDF (Term Frequency-Inverse\n",
    " Document Frequency) for a list of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.40352536 0.         0.         0.53058735 0.40352536 0.\n",
      "  0.         0.         0.62674687]\n",
      " [0.34101521 0.         0.         0.         0.         0.44839402\n",
      "  0.44839402 0.44839402 0.52965746]\n",
      " [0.         0.50165133 0.50165133 0.         0.38151877 0.\n",
      "  0.         0.         0.59256672]]\n",
      "Feature Names:\n",
      "['cat' 'dog' 'fog' 'hat' 'in' 'mat' 'on' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"the cat in the hat\",\n",
    "        \"the cat sat on the mat\",\n",
    "        \"the dog in the fog\"\n",
    "    ]\n",
    "    \n",
    "    tfidf_matrix, feature_names = compute_tfidf(documents)\n",
    "    print(\"TF-IDF Matrix:\")\n",
    "    print(tfidf_matrix.toarray())\n",
    "    print(\"Feature Names:\")\n",
    "    print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an algorithm to find the principal components of a dataset using\n",
    " singular value decomposition (SVD), which is a core part of PCA (Principal Component\n",
    " Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      "[[-0.6778734  -0.73517866]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca_svd(X, n_components):\n",
    "    X_meaned = X - np.mean(X, axis=0)\n",
    "    U, S, Vt = np.linalg.svd(X_meaned, full_matrices=False)\n",
    "    principal_components = Vt[:n_components]\n",
    "    return principal_components\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    X = np.array([[2.5, 2.4],\n",
    "                  [0.5, 0.7],\n",
    "                  [2.2, 2.9],\n",
    "                  [1.9, 2.2],\n",
    "                  [3.1, 3.0],\n",
    "                  [2.3, 2.7],\n",
    "                  [2, 1.6],\n",
    "                  [1, 1.1],\n",
    "                  [1.5, 1.6],\n",
    "                  [1.1, 0.9]])\n",
    "    \n",
    "    n_components = 1\n",
    "    principal_components = pca_svd(X, n_components)\n",
    "    print(\"Principal Components:\")\n",
    "    print(principal_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a Python function to calculate the AUC-ROC score for a binary\n",
    " classification problem, given the true labels and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_auc_roc(y_true, y_scores):\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example true labels and predicted probabilities\n",
    "    y_true = [0, 1, 1, 0, 1]\n",
    "    y_scores = [0.1, 0.4, 0.35, 0.8, 0.7]\n",
    "    \n",
    "    auc_roc = calculate_auc_roc(y_true, y_scores)\n",
    "    print(\"AUC-ROC score:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Python function to apply dropout regularization to the neurons of a\n",
    " given neural network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer output after applying dropout:\n",
      "[[0.  1.6 0. ]\n",
      " [0.8 1.8 0. ]\n",
      " [0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_dropout(layer_output, dropout_rate):\n",
    "    if dropout_rate < 0 or dropout_rate >= 1:\n",
    "        raise ValueError(\"Dropout rate must be between 0 and 1.\")\n",
    "    \n",
    "    mask = np.random.binomial(1, 1 - dropout_rate, size=layer_output.shape)\n",
    "    return layer_output * mask / (1 - dropout_rate)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example layer output\n",
    "    layer_output = np.array([[0.2, 0.8, 0.5],\n",
    "                             [0.4, 0.9, 0.3],\n",
    "                             [0.6, 0.2, 0.1]])\n",
    "    \n",
    "    dropout_rate = 0.5\n",
    "    output_with_dropout = apply_dropout(layer_output, dropout_rate)\n",
    "    print(\"Layer output after applying dropout:\")\n",
    "    print(output_with_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a Python function to perform feature scaling using standardization (z-score\n",
    " normalization), which transforms features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized features:\n",
      "[[-1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_features(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    standardized_X = (X - mean) / std\n",
    "    return standardized_X\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    X = np.array([[1, 2],\n",
    "                  [2, 3],\n",
    "                  [3, 4],\n",
    "                  [4, 5],\n",
    "                  [5, 6]])\n",
    "    \n",
    "    standardized_X = standardize_features(X)\n",
    "    print(\"Standardized features:\")\n",
    "    print(standardized_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement a Python function to compute the cross-entropy loss for a multi-class\n",
    " classification problem, given the true labels and predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 0.3635480396729762\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    num_samples = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / num_samples\n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example true labels (one-hot encoded) and predicted probabilities\n",
    "    y_true = np.array([[1, 0, 0],\n",
    "                       [0, 1, 0],\n",
    "                       [0, 0, 1]])\n",
    "    y_pred = np.array([[0.7, 0.2, 0.1],\n",
    "                       [0.1, 0.8, 0.1],\n",
    "                       [0.2, 0.2, 0.6]])\n",
    "    \n",
    "    loss = cross_entropy_loss(y_true, y_pred)\n",
    "    print(\"Cross-entropy loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
